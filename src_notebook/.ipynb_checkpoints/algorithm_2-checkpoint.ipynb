{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae47064",
   "metadata": {},
   "source": [
    "# Algorithm 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e934715",
   "metadata": {},
   "source": [
    "### PART 1 -  \n",
    "#### Execute KMeans. \n",
    "Try to find automatticaly the 'elbow' - the best K results ( withour relating to the daily hours or to the number of resources).\n",
    "\n",
    "### PART 2 - \n",
    "#### Order The Clusters.\n",
    " * 2a- Index all datapoints (jobs) in a cluster with the same index, when indexing is according to the distance between the clusters.\n",
    " * 2b- Greedy - Execute Shortest Path algorithm at each cluster, connect the cluster by the closest datapoints (jobs). (Index each datapoint with different index)\n",
    " * 2c - small clusters - execute cluster of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dc064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np # used for scientific computing\\n\",\n",
    "import pandas as pd # used for data analysis and manipulation\\n\",\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\\n\",\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "from sklearn.cluster import KMeans \n",
    "import heapq\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b47d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_file = 'second_datadet'\n",
    "# export_results = True => export excel with results. otherwise not.\n",
    "# Give the option to analyze the algorithms without exporting the results\n",
    "export_results = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6475e761",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/second_dataset.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77810cad5a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../Data/{name_of_file}.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1072\u001b[0m                     \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(path, content, storage_options)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mcontent_or_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     ) as handle:\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/second_dataset.xlsx'"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "df = pd.read_excel(f'../Data/{name_of_file}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036db272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(c): \n",
    "    plt.scatter(df['Latitude'], df['Longitude'], color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of the jobs\n",
    "plot('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b15e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the relvent columns\n",
    "dataset = np.array(df[['Latitude', 'Longitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79082e",
   "metadata": {},
   "source": [
    "# PART 1 - KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc911c4",
   "metadata": {},
   "source": [
    "finding the elbow is the method to find the best K. \n",
    "The solution is not optimal which means it is not return the best result any time.\n",
    "In contrast, the result will be always close to the truth and the small error may not affect on our results drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c008e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_the_elbow(inertia_lst):\n",
    "    \"\"\"\n",
    "    Finding the elbow - means to find the best K for the K means according to the inertia.\n",
    "    The optimal solution is available only manually. \n",
    "    This function return a good K automatically.(not always the optimal) \n",
    "    \"\"\"\n",
    "    length = len(inertia_lst) - 1\n",
    "\n",
    "    # delta_1: index=0 -> #cluster=2 -> best_k_idx=1\n",
    "    delta_1 = [inertia_lst[i] - inertia_lst[i+1] for i in range(length)]\n",
    "\n",
    "    # delta_2: index 0 -> #cluster=3 -> best_k_idx=2\n",
    "    delta_2 = [delta_1[i] - delta_1[i+1] for i in range(length - 1)]\n",
    "\n",
    "    strength = [(i, delta_2[i+1] - delta_1[i+2]) for i in range(length - 2) if delta_1[i+1] >= 0 and delta_2[i+1] >= 0]\n",
    "\n",
    "    best_k_idx = max(strength,key=lambda t: t[1])[0] + 2\n",
    "    return best_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745853b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_k(Ks):\n",
    "    \"\"\"\n",
    "    Choosing a good k from the given list of Ks\n",
    "    Return - \n",
    "    - best_k_idx\n",
    "    - inertia_lst - to represent that choosing make sense\n",
    "    \"\"\"\n",
    "    # execute KMeans for each K in Ks\n",
    "    inertia_lst = []\n",
    "    for k in Ks:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        # fit the data\n",
    "        _ = kmeans.fit_predict(dataset)\n",
    "        # save inertia\n",
    "        inertia_lst.append(kmeans.inertia_)\n",
    "    \n",
    "    best_k_idx = finding_the_elbow(inertia_lst)\n",
    "    \n",
    "    return best_k_idx, inertia_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K to check for the KMeans\n",
    "Ks = [k for k in range(4, 150)]\n",
    "best_k_idx, inertia_lst = choose_k(Ks)\n",
    "best_k = Ks[best_k_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inertia as function of number of clusers (k)\n",
    "plt.plot(Ks, inertia_lst, marker=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a88f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best KMeans , n_clusters=best_k\n",
    "best_kmeans = KMeans(n_clusters=best_k, random_state=0)\n",
    "# get labels \n",
    "labels = best_kmeans.fit_predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bc613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_instances_to_clusters(dataset, labels, best_k):\n",
    "    \"\"\"\n",
    "    return dictionary {key = cluster_number , value = list of instances in this cluster}\n",
    "    \"\"\"\n",
    "    # init dictionary\n",
    "    dict_of_clusters = {cluster: [] for cluster in range(best_k)}\n",
    "\n",
    "    for idx, instance in enumerate(dataset):\n",
    "        dict_of_clusters[labels[idx]].append(instance)\n",
    "    \n",
    "    return dict_of_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c705adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_clusters = seperate_instances_to_clusters(dataset, labels, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cluster sizes\n",
    "for cluster, instances in dict_of_clusters.items():\n",
    "    print(f'*Cluster {cluster}: {len(instances)} jobs.')\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8634f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "clusters = [c for c in range(best_k)]\n",
    "colors = cm.rainbow(np.linspace(0, 1, Ks[3] * 5))\n",
    "colors = colors[::5]\n",
    "for cluster, c in zip(clusters, colors):\n",
    "    data = np.array(dict_of_clusters[cluster])\n",
    "    plt.scatter(data[:, 0], data[:, 1], color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145979b2",
   "metadata": {},
   "source": [
    "# PART 2 - Order The Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers (centroids) of clusters\n",
    "centers = best_kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radius of the Earth\n",
    "R = 6373.0\n",
    "\n",
    "def compute_distance(place_1, place_2):\n",
    "    \"\"\"\n",
    "    compute the distance between two lat-long coordinates\n",
    "\n",
    "    Input:\n",
    "\n",
    "    place_1 - tuple : (latitude, longitude) of place 1\n",
    "    place_2 - tuple : (latitude, longitude) of place 2\n",
    "\n",
    "    Returns:\n",
    "    distance - float number \n",
    "    \"\"\"    \n",
    "    \n",
    "    distance_lat = place_2[0] - place_1[0]\n",
    "    distance_long = place_2[1] - place_1[1]\n",
    "\n",
    "    #Haverinse formula \n",
    "    x = math.sin(distance_lat / 2)**2 + math.cos(place_1[0]) * math.cos(place_2[0]) * math.sin(distance_long / 2)**2\n",
    "\n",
    "    y = 2 * math.atan2(math.sqrt(x), math.sqrt(1 - x))\n",
    "\n",
    "    return R * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2203947",
   "metadata": {},
   "source": [
    "### 2a -\n",
    "- Order the cluster according to the distances between them. \n",
    "- All data points in the same cluster will get the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3061dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the clusters according to distance from cluster 0 \n",
    "\n",
    "d_array = [(0,0)]\n",
    "c_0 = centers[0]\n",
    "for idx, c in enumerate(centers[1:]):\n",
    "    d_array += [(compute_distance(c_0, c), idx + 1)]\n",
    "cluster_distance = sorted(d_array, key=lambda x: x[0]) \n",
    "\n",
    "dict_cluster_to_idx = {c[1] : idx for idx, c in enumerate(cluster_distance)}\n",
    "\n",
    "idx_res_column = []\n",
    "\n",
    "# for each row add the relevent number according to the order of the clusters\n",
    "for label in labels:\n",
    "    idx_res_column += [dict_cluster_to_idx[label]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the column to the dataFrame\n",
    "df['IDC_Index__c'] = idx_res_column\n",
    "# save a copy of 2a results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2a_dataframe = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete result new excel\n",
    "if export_results:\n",
    "    df.to_excel(f'results_{name_of_file}_(2a).xlsx', sheet_name='results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483cfbb1",
   "metadata": {},
   "source": [
    "### 2b - \n",
    "Order the cluters by executing shortest path algorithm on each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33acd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seprate the datapoints according to their cluster\n",
    "# save id = job's row.\n",
    "separate_dataset = [[] for cluster in range(best_k)]\n",
    "id = 0\n",
    "for datapoint, label in zip(dataset, labels):\n",
    "    separate_dataset[label].append((id, datapoint))\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923ec8e",
   "metadata": {},
   "source": [
    "Greedy - on each cluster index according to the the distance from the first job in the cluster\n",
    "Shoretest path Algorithm: \n",
    "### Dijkstra algorithm\n",
    "\n",
    "- We execute SP on every cluster according to the order we achieved above.\n",
    "- Fisrt iteration: \n",
    "    - Startpoint: take random datapoint\n",
    "    - Endpoint: random datapoint from the next cluster\n",
    "- Next iterations:\n",
    "    - StartPoint: the endpoint from the previous iteration\n",
    "    - Endpoint: random datapoint from the next cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c972024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the oredr of the clusters\n",
    "cluster_order = [c[1] for c in cluster_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VALUE = sys.float_info.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex: \n",
    "    \n",
    "    def __init__(self, job, is_root=False):\n",
    "        self.id = job[0]\n",
    "        self.place = job[1]\n",
    "        self.distance = 0 if (is_root) else MAX_VALUE\n",
    "        self.parent = None\n",
    "\n",
    "\n",
    "    def relax(self, v):\n",
    "        distance_from_self_to_v = compute_distance(self.place, v.place) \n",
    "        if v.distance > self.distance + distance_from_self_to_v:\n",
    "            v.distance = self.distance + distance_from_self_to_v\n",
    "            v.parent = self.id\n",
    "            \n",
    "    # implemt =, < for using heapq on vertex\n",
    "    def __eq__(self, other):\n",
    "        if self.distance == other.distance:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.distance < other.distance:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb63cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "results = []\n",
    "for cluster in range(best_k):\n",
    "    root = Vertex(separate_dataset[cluster][0], is_root=True)\n",
    "    G = [Vertex(datapoint) for datapoint in separate_dataset[cluster][1:]]\n",
    "    G.append(root)\n",
    "    # we do not use E since there are edges between all the vertices\n",
    "    #E = [(v1.id, v2.id) for v1 in G for v2 in G]\n",
    "\n",
    "    # ------- Execute DIJKSTRA -------- #\n",
    "    priority_queue = []\n",
    "    #push all vertices\n",
    "    for v in G:\n",
    "        heapq.heappush(priority_queue, v)\n",
    "    while priority_queue != []:\n",
    "        # dequeue the smallest vertex (according to the distance field)\n",
    "        u = heapq.heappop(priority_queue)\n",
    "        results.append((u.id, idx)) #u.index = idx  ->. at Dijkstra the first to leave th queue is the closet to the begining\n",
    "        for v in G:\n",
    "            u.relax(v)\n",
    "        idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort result according to rows order ('id')\n",
    "results.sort(key=lambda row: row[0])\n",
    "results = [row[1] for row in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the column to the dataFrame\n",
    "df['IDC_Index__c'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete result new excel\n",
    "if export_results:\n",
    "    df.to_excel(f'results_{name_of_file}_(2b).xlsx', sheet_name='results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8d2d6",
   "metadata": {},
   "source": [
    "### 2c -\n",
    "- For each cluster perform Kmeans inside him and create sub-cluster\n",
    "- Order the cluster according to the distances between them. \n",
    "- All data points in the same sub-cluster will get the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel(f'results_{name_of_file}_(2a).xlsx')\n",
    "df = results_2a_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0209e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K to check for the KMeans\n",
    "Ks = [k for k in range(2, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cf581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the warning of dupliacte objects - since there are data points on the same location\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "dict_of_clusters_2 = dict({})\n",
    "index_2c = 0\n",
    "r = 1\n",
    "\n",
    "for cluster in clusters:        \n",
    "    dataset = dict_of_clusters[cluster]\n",
    "    inertia_lst = []\n",
    "    if(len(dataset) > 10):\n",
    "        \n",
    "        best_k_idx, _ = choose_k(Ks)\n",
    "        best_k = Ks[best_k_idx]\n",
    "        \n",
    "        # get the best KMeans , n_clusters=best_k\n",
    "        best_kmeans = KMeans(n_clusters=best_k, random_state=1)\n",
    "        \n",
    "        # get labels \n",
    "        labels = best_kmeans.fit(dataset, sample_weight=1).labels_\n",
    "\n",
    "        # seperate the instances to clusters\n",
    "        dict_of_clusters_2c = seperate_instances_to_clusters(dataset, labels, best_k)\n",
    "        \n",
    "        for cluster, instances in dict_of_clusters_2.items():\n",
    "            dict_of_clusters_2c.update({index_2c: instances}) \n",
    "            index_2c += 1\n",
    "    \n",
    "    else:\n",
    "        dict_of_clusters_2c.update({index_2c: dataset}) \n",
    "        index_2c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster, instances in dict_of_clusters_2c.items():\n",
    "    for instance in instances:\n",
    "        inds = np.where(np.logical_and(df['Latitude'].values  == instance[0],df['Longitude'].values == instance[1]))[0]\n",
    "        df['IDC_Index__c'].iloc[inds] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea08332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete result new excel\n",
    "if export_results:\n",
    "    df.to_excel(f'results_{name_of_file}_(2c).xlsx', sheet_name='results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ef00b",
   "metadata": {},
   "source": [
    "# 2d\n",
    "\n",
    "##### on each cluster from 2a execute 1-TSP algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-opt Algorithm adapted from https://en.wikipedia.org/wiki/2-opt\n",
    "\n",
    "# Calculate the euclidian distance in n-space of the route r traversing jobs j, ending at the path start.\n",
    "path_distance = lambda r,j: np.sum([np.linalg.norm(j[r[p]] - j[r[p - 1]]) for p in range(len(r))])\n",
    "\n",
    "# Reverse the order of all elements from element i to element k in array r.\n",
    "two_opt_swap = lambda r,i,k: np.concatenate((r[0:i], r[k:- len(r) + i - 1:-1],r[k + 1:len(r)]))\n",
    "\n",
    "def two_opt(jobs,improvement_threshold): \n",
    "    route = np.arange(jobs.shape[0])\n",
    "    # Initialize the improvement factor.\n",
    "    improvement_factor = 1 \n",
    "    best_distance = path_distance(route,jobs)\n",
    "    \n",
    "    while improvement_factor > improvement_threshold: \n",
    "        # Record the distance at the beginning of the loop.\n",
    "        distance_to_beat = best_distance\n",
    "        \n",
    "        for swap_first in range(1,len(route)-2):\n",
    "            for swap_last in range(swap_first+1,len(route)): \n",
    "                # try reversing the order of these jobs\n",
    "                new_route = two_opt_swap(route,swap_first,swap_last) \n",
    "                # check the total distance with this modification.\n",
    "                new_distance = path_distance(new_route,jobs)\n",
    "                \n",
    "                if new_distance < best_distance: \n",
    "                    route = new_route\n",
    "                    best_distance = new_distance \n",
    "        # Calculate how much the route has improved.\n",
    "        improvement_factor = 1 - best_distance/distance_to_beat \n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf24359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-TSP\n",
    "def one_TSP(data, offset=0):\n",
    "    \"\"\"\n",
    "    Execute 1-TSP optimal algorithm on the give sub-graph_i\n",
    "    \n",
    "    Input:\n",
    "    Gi - sub-graph i\n",
    "    offset - the index to start from.\n",
    "    \"\"\"\n",
    "    R = 6371\n",
    "    \n",
    "    lat = data[\"Latitude\"].map(math.radians)\n",
    "    lon = data[\"Longitude\"].map(math.radians)\n",
    "    x = lon.map(math.cos)*lat.map(math.cos)*R\n",
    "    y = lon.map(math.cos)*lat.map(math.sin)*R\n",
    "\n",
    "    data[\"lat_radians\"] = lat\n",
    "    data[\"lon_radians\"] = lon\n",
    "    data[\"x\"] = x\n",
    "    data[\"y\"] = y\n",
    "    \n",
    "    jobs_location = data[['x', 'y']].copy()\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 100), copy=True)\n",
    "    scaled_df = scaler.fit_transform(jobs_location)\n",
    "    scaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2'])\n",
    "    \n",
    "    jobs_location = np.asarray(jobs_location)\n",
    "    scaled = np.asarray(scaled_df)\n",
    "    \n",
    "    route = two_opt(scaled, 0.001)\n",
    "    \n",
    "    return route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c992e",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "###### we remove the jobs with the same location and give them the same index after that, then the run time of 1-TSP iis faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_the_same_location(place_1, place_2):\n",
    "    return True if place_1['Latitude'] == place_2['Latitude'] and place_1['Longitude'] == place_2['Longitude'] else False\n",
    "\n",
    "\n",
    "def merge_duplicates(df):\n",
    "    duplicates_indexes = []\n",
    "    dict_of_duplicates = dict({})\n",
    "    res = df.copy()\n",
    "    for index_1, row_1 in df.iterrows():\n",
    "        if index_1 not in duplicates_indexes:\n",
    "            current_duplicates = []\n",
    "            for index_2, row_2 in df.iterrows():\n",
    "                if index_1 < index_2 and index_2 not in duplicates_indexes and is_the_same_location(row_1, row_2):\n",
    "                        res.drop(index=index_2, axis=0, inplace=True)\n",
    "                        current_duplicates.append(index_2)\n",
    "            duplicates_indexes += current_duplicates\n",
    "            dict_of_duplicates[index_1] = current_duplicates\n",
    "    return res, dict_of_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7816e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(f'results_{name_of_file}_(2a).xlsx')\n",
    "df = results_2a_dataframe.copy()\n",
    "# get IDC_index column number \n",
    "idc_index_col_number = [i for i in range(len(df.columns)) if df.columns[i] == 'IDC_Index__c'][0]\n",
    "unique_df, dict_of_duplicates = merge_duplicates(df)\n",
    "num_of_clusters = max(unique_df['IDC_Index__c'])\n",
    "unique_df = unique_df[['Latitude', 'Longitude', 'IDC_Index__c']]\n",
    "data = pd.DataFrame({})\n",
    "offset = 0\n",
    "for i in range(num_of_clusters):\n",
    "    current_df_cluster = unique_df[unique_df['IDC_Index__c'] == i].copy()\n",
    "    indexes = list(current_df_cluster.index)\n",
    "    results = one_TSP(current_df_cluster, offset)\n",
    "    # handle same place case -> results do not consider it  \n",
    "    for idx, res in zip(indexes, results):\n",
    "        lat = unique_df['Latitude'][idx]\n",
    "        lon = unique_df['Longitude'][idx]\n",
    "        df.iloc[idx, idc_index_col_number] = res + offset\n",
    "    offset += current_df_cluster.shape[0]\n",
    "\n",
    "for key, values in dict_of_duplicates.items():\n",
    "    if values:\n",
    "        for value_idx in values:\n",
    "            df.iloc[value_idx, idc_index_col_number] = df.iloc[key, idc_index_col_number] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete result new excel\n",
    "# if export_results:\n",
    "df.to_excel(f'results_{name_of_file}_(2d).xlsx', sheet_name='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7fcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
