{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1d2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np # used for scientific computing\\n\",\n",
    "import pandas as pd # used for data analysis and manipulation\\n\",\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\\n\",\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6b5f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get these 2 as an input\n",
    "name_of_file = 'third_dataset'\n",
    "number_of_resources = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632a89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data \n",
    "df = pd.read_excel(f'../Data/{name_of_file}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb71b81",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "###### we remove the jobs with the same location and give them the same index after that, then the run time of 1-TSP iis faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374ac2a",
   "metadata": {},
   "source": [
    "### If we calculate the correct driving time between pairs of jobs, this algorithm may be the best solution.\n",
    "### We recommend trying it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ff26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_the_same_location(place_1, place_2):\n",
    "    return True if place_1['Latitude'] == place_2['Latitude'] and place_1['Longitude'] == place_2['Longitude'] else False\n",
    "\n",
    "\n",
    "def merge_duplicates(df):\n",
    "    duplicates_indexes = []\n",
    "    dict_of_duplicates = dict({})\n",
    "    res = df.copy()\n",
    "    for index_1, row_1 in df.iterrows():\n",
    "        if index_1 not in duplicates_indexes:\n",
    "            current_duplicates = []\n",
    "            for index_2, row_2 in df.iterrows():\n",
    "                if index_1 < index_2 and index_2 not in duplicates_indexes and is_the_same_location(row_1, row_2):\n",
    "                        res.drop(index=index_2, axis=0, inplace=True)\n",
    "                        current_duplicates.append(index_2)\n",
    "            duplicates_indexes += current_duplicates\n",
    "            dict_of_duplicates[index_1] = current_duplicates\n",
    "    return res, dict_of_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dddea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique, dict_of_duplicates = merge_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    \"\"\"\n",
    "    Vertex on the graph G is a job with:\n",
    "    id: index in the given excel (in order to update it well),\n",
    "    location - latitude, longitude,\n",
    "    expect duration time in minutes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id, lat, lon): # duration_time\n",
    "        self.id = id\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        self.index = -1\n",
    "        # self.weight = duration_time\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'({self.lat}, {self.lon}, {self.weight})'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd24f4",
   "metadata": {},
   "source": [
    "# Note \n",
    "\n",
    "* we do not calculate the driving distance. \n",
    "* we assume that the speed in any location is the same. Which is probably wrong.\n",
    "\n",
    "Results may improve by changing the functions compute_distance, proxy_time_between_u_v to functions that estimate time according to the driving way\n",
    "\n",
    "\n",
    "##### It is enough to change these calculation only in the Edge class  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge:\n",
    "    \"\"\"\n",
    "    Edge is connect between two vetices.(Jobs)\n",
    "    The weight include also the duration time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, u, v):\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        # Each vertex is incident to exactly two edges of the cycle, so we can define the weight of an edge as:\n",
    "        self.weight =   self.proxy_time_between_u_v()\n",
    "        # 0.5 * (u.weight + v. weight)\n",
    "        \n",
    "    def proxy_time_between_u_v(self, kmh=7):\n",
    "        \"\"\"\n",
    "        return the time (under assumption) to go from u to v\n",
    "        \"\"\"\n",
    "        d = self.compute_distance()\n",
    "        \n",
    "        return d # / kmh \n",
    "    \n",
    "    \n",
    "    def compute_distance(self):\n",
    "        \"\"\"\n",
    "        compute the distance (km) between two lat-long coordinates\n",
    "\n",
    "        \"\"\"    \n",
    "        # radius of the Earth\n",
    "        R = 6373.0\n",
    "\n",
    "        distance_lat = self.u.lat - self.v.lat\n",
    "        distance_long = self.u.lon - self.v.lon\n",
    "\n",
    "        #Haverinse formula \n",
    "        x = math.sin(distance_lat / 2)**2 + math.cos(self.v.lat) * math.cos(self.u.lat) * math.sin(distance_long / 2)**2\n",
    "\n",
    "        y = 2 * math.atan2(math.sqrt(x), math.sqrt(1 - x))\n",
    "\n",
    "        return (R * y) # / 1000\n",
    "    \n",
    "    \n",
    "    def consist_vertex(self, v):\n",
    "        return True if (self.u == v or self.v == v) else False\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'[{self.u}, {self.v}] , w(e) = {self.weight}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14741949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_i:\n",
    "    \"\"\"\n",
    "    Create a subgraph G_i\n",
    "    \"\"\"\n",
    "    def __init__(self, all_vertices, V_i, E_i):\n",
    "        self.V = V_i \n",
    "        self.length = len(V_i)\n",
    "        self.E = E_i\n",
    "        self.weight = sum(edge.weight for edge in E_i) \n",
    "        margin_weight_dict = {v.id: (v, self.init_margin_weight(v)) for v in all_vertices}\n",
    "        # sorted dictionary\n",
    "        self.margin_weight_dict = dict(sorted(margin_weight_dict.items(), key=lambda item: item[1][1], reverse=True))\n",
    "        self.size = (2 / (self.length - 1))  * (self.weight)  \n",
    "    \n",
    "    \n",
    "    def init_margin_weight(self, v):\n",
    "        \"\"\"\n",
    "        margin_weight(v) = sum of w(v', v) for v' in V_i\n",
    "        \"\"\"\n",
    "        relevant_edges = [Edge(u, v) for u in self.V]\n",
    "        return sum(edge.weight for edge in relevant_edges)\n",
    "    \n",
    "    \n",
    "    def get_size_after_transfer_remove(self, v):\n",
    "        \"\"\"\n",
    "        return the size if the remove operation will be executed\n",
    "        \"\"\"\n",
    "        if self.length == 0:\n",
    "            return sys.float_info.max \n",
    "\n",
    "        if self.length == 1:\n",
    "            return 0\n",
    "        \n",
    "        if self.length == 2:\n",
    "            return self.V[0].weight if self.V[0] == v else self.V[1].weight\n",
    "        \n",
    "        return (2 / (self.length - 2)) * (self.weight - self.margin_weight_dict.get(v.id)[1]) \n",
    "        \n",
    "    \n",
    "    def get_size_after_transfer_add(self, v):\n",
    "        \"\"\"\n",
    "        return the size if the add operation will be executed\n",
    "        \"\"\"\n",
    "        if self.length == 0:\n",
    "            return v.weight\n",
    "        \n",
    "        return (2 / self.length) * (self.weight + self.margin_weight_dict.get(v.id)[1])\n",
    "    \n",
    "    \n",
    "    def get_size_after_swap(self, v, u):\n",
    "        \"\"\"\n",
    "        return the size if the swap operation will be executed such that v in V_i and u is from another sub-graph\n",
    "        \"\"\"\n",
    "        return self.weight - self.margin_weight_dict.get(v.id)[1] + self.margin_weight_dict.get(u.id)[1] - Edge(u, v).weight\n",
    "    \n",
    "    \n",
    "    def add_vertex(self, v):\n",
    "        \"\"\"\n",
    "        Add vertex to the graph_i.\n",
    "        Update the the fields as needed.\n",
    "        \"\"\"\n",
    "        self.V += [v]\n",
    "        self.length += 1\n",
    "        self.weight += self.margin_weight_dict.get(v.id)[1]\n",
    "        for key, value in self.margin_weight_dict.items():\n",
    "            self.margin_weight_dict[key] = (value[0], value[1] + Edge(v, value[0]).weight) if self.length > 1 else v.weight\n",
    "        self.size = (2 / (self.length - 1))  * (self.weight) \n",
    "        \n",
    "        \n",
    "    def remove_vertex(self, v):\n",
    "        \"\"\"\n",
    "        Remove vertex from graph_i.\n",
    "        Update the the fields as needed.\n",
    "        \"\"\"\n",
    "        for idx, u in enumerate(self.V):\n",
    "            if u == v:\n",
    "                _ = self.V.pop(idx)\n",
    "        self.weight -= self.margin_weight_dict.get(v.id)[1]\n",
    "        for key, value in self.margin_weight_dict.items():\n",
    "            self.margin_weight_dict[key] = (value[0], value[1] - Edge(v, value[0]).weight)\n",
    "        self.length -= 1\n",
    "        self.size = (2 / (self.length - 1))  * (self.weight) if self.length > 1 else self.V[0].weight\n",
    "     \n",
    "    \n",
    "    def swap(self, v, u):\n",
    "        \"\"\"\n",
    "        Swap vertices between graph_i and graph_j such that i != j.\n",
    "        Remove v from graph_i and add u to graph_i.\n",
    "        \n",
    "        Note: in order to execute the swap coorrectly - we need to execute it on both graphs (aka - Gi, Gj)\n",
    "        \n",
    "        Input:\n",
    "        - v: v in graph_i \n",
    "        - u: u in graph_j such that u in all_vertices\n",
    "        \"\"\"\n",
    "        self.add_vertex(u)\n",
    "        self.remove_vertex(v)\n",
    "        \n",
    "        \n",
    "    def get_jobs_location(self):\n",
    "        \"\"\"\n",
    "        return latitude list and longitude list of the vertices in graph_i\n",
    "        \"\"\"\n",
    "        lat_lst = [v.lat for v in self.V]\n",
    "        lon_lst = [v.lon for v in self.V]\n",
    "        return lat_lst, lon_lst\n",
    "    \n",
    "    \n",
    "    def plot_sub_graph(self):\n",
    "        lat, lon = self.get_jobs_location()\n",
    "        plt.scatter(lat, lon)\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_partition(G, number_of_resources):\n",
    "    \"\"\"\n",
    "    Initiate a Partiton P=[G1, G2, ..., Gm] where m = number of resources(workers)\n",
    "    \"\"\"\n",
    "    V = G[0]\n",
    "    Gs = []\n",
    "\n",
    "    number_of_jobs = len(V)\n",
    "    Gi_size = number_of_jobs // number_of_resources\n",
    "    remider = number_of_jobs % number_of_resources\n",
    "    v_index = 0 \n",
    "    for i in range(number_of_resources):\n",
    "        length = Gi_size + 1 if remider > i else Gi_size\n",
    "        V_i = [V[j] for j in range(v_index, length + v_index)]\n",
    "        v_index += length\n",
    "        E_i = [Edge(V_i[j1], V_i[j2]) for j1 in range(length) for j2 in range(j1 + 1, length)]\n",
    "        Gs.append(Graph_i(V, V_i, E_i)) \n",
    "    \n",
    "    plot_partition(Gs, 'init Partition')\n",
    "    return Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e49e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 1\n",
    "\n",
    "def find_best_transfer(G1, G2):\n",
    "    \"\"\"\n",
    "    find the best vertices to execute transfer from G1 to G2 \n",
    "    return None if there is not exists such a transfer \n",
    "    \"\"\"\n",
    "    max_size = max(G1.size, G2.size)\n",
    "    best_v1 = None\n",
    "    \n",
    "    for v1 in G1.V:\n",
    "        \n",
    "        current_size = max(G1.get_size_after_transfer_remove(v1), G2.get_size_after_transfer_add(v1))\n",
    "        if current_size < max_size:\n",
    "            max_size = current_size\n",
    "            best_v1 = v1\n",
    "    \n",
    "    return None if best_v1 is None else best_v1\n",
    "\n",
    "\n",
    "def find_best_swap(G1, G2):\n",
    "    \"\"\"\n",
    "    find the best vertices to execute swap between G1 and G2 \n",
    "    return None if there is not exists such a swap \n",
    "    \"\"\"\n",
    "    max_size = max(G1.size, G2.size)\n",
    "    best_v1 = None\n",
    "    best_v2 = None\n",
    "    \n",
    "    for v1 in G1.V:\n",
    "        for v2 in G2.V:\n",
    "            current_size = max(G1.get_size_after_swap(v1, v2), G2.get_size_after_swap(v2, v1))\n",
    "            if (current_size < max_size):\n",
    "                max_size = current_size\n",
    "                best_v1 = v1\n",
    "                best_v2 = v2\n",
    "    \n",
    "    return None if best_v1 is None else best_v1, best_v2\n",
    "\n",
    "    \n",
    "def improve_partition(Gs):\n",
    "    \"\"\"\n",
    "    Algorithm 1 - improve partition by transfer and swaps vertices \n",
    "    \"\"\"\n",
    "    # num_of_resources == len(Gs) => True\n",
    "    num_of_resources = len(Gs)\n",
    "    unchecked_sub_graphs_pairs = [(i, j) for i in range(num_of_resources) for j in range(num_of_resources) if i != j]\n",
    "    unchecked_transfers = [(i, j) for i in range(num_of_resources) for j in range(num_of_resources) if Gs[i].size > Gs[j].size]\n",
    "    \n",
    "    while len(unchecked_sub_graphs_pairs) > 0:\n",
    "        current_transfer = unchecked_sub_graphs_pairs[0]\n",
    "        G1 = Gs[current_transfer[0]]\n",
    "        G2 = Gs[current_transfer[1]]\n",
    "        \n",
    "        if current_transfer in unchecked_transfers:\n",
    "            v1 = find_best_transfer(G1, G2)\n",
    "            if v1:\n",
    "                G1.remove_vertex(v1)\n",
    "                G2.add_vertex(v1)                \n",
    "            else:                \n",
    "                unchecked_transfers.remove(current_transfer)\n",
    "        else:  \n",
    "            v1, v2 = find_best_swap(G1, G2)\n",
    "            if v1:\n",
    "                G1.swap(v1, v2)\n",
    "                G2.swap(v2, v1)                \n",
    "            else:\n",
    "                unchecked_sub_graphs_pairs.pop(0)\n",
    "    return Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2\n",
    "\n",
    "def find_minizer_of_margin(Gs, v):\n",
    "    \"\"\"\n",
    "    Find the index of the sub-graph_i in Gs such that margin_weight(Gi, v) is minimal. \n",
    "    \"\"\"\n",
    "    min_idx = 0\n",
    "    for idx, G in enumerate(Gs):\n",
    "        if G.margin_weight_dict.get(v.id)[1] < Gs[min_idx].margin_weight_dict.get(v.id)[1]:\n",
    "            min_idx = idx\n",
    "    return idx\n",
    "\n",
    "def transfer_outliers(Gs, alpha=1.5):\n",
    "    \"\"\"\n",
    "    Algorithm 2 - transfer ouliers to their compatible graph\n",
    "    Input:\n",
    "    Gs - partition of graph G\n",
    "    alpha - control the number of detected outliers which decrease as long as alpha increase, need to be bigger than 1.  \n",
    "    \"\"\"\n",
    "    outliers = []\n",
    "    for idx, G in enumerate(Gs):\n",
    "        for v in G.V:\n",
    "            if G.margin_weight_dict.get(v.id)[1] > (alpha * 2 * G.weight) / G.length :\n",
    "                new_G_idx = find_minizer_of_margin(Gs, v)\n",
    "                if new_G_idx != idx:\n",
    "                    outliers.append((v, idx, new_G_idx))\n",
    "    \n",
    "    for t in outliers:\n",
    "        Gs[idx].remove_vertex(v)\n",
    "        Gs[new_G_idx].add_vertex(v)\n",
    "        \n",
    "    return Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 3 \n",
    "def average_hamilton_partition(G, number_of_resources):\n",
    "    \"\"\"\n",
    "    Algorithm 3 - Partition a graph G by two alternating phasesâ€” \n",
    "        improvement (Algorithm 1) and transferring outliers (Algorithm 2)\n",
    "    \"\"\"\n",
    "    Gs = get_random_partition(G, number_of_resources)\n",
    "    _ = improve_partition(Gs)\n",
    "    Ca = max([G.size for G in Gs])\n",
    "    new_Ca = Ca\n",
    "    count = 0\n",
    "    while count < 5:\n",
    "        print(f'Ca = {Ca}')\n",
    "        new_Gs = Gs.copy()\n",
    "        _ = transfer_outliers(new_Gs, 1.005)\n",
    "        _ = improve_partition(new_Gs)\n",
    "        new_Ca = max([G.size for G in new_Gs])\n",
    "        print(f'new_Ca =  {new_Ca}')\n",
    "        if Ca > new_Ca:\n",
    "            Gs = new_Gs\n",
    "            Ca = new_Ca\n",
    "        else:\n",
    "            count += 1\n",
    "    return Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-opt Algorithm adapted from https://en.wikipedia.org/wiki/2-opt\n",
    "\n",
    "# Calculate the euclidian distance in n-space of the route r traversing jobs j, ending at the path start.\n",
    "path_distance = lambda r,j: np.sum([np.linalg.norm(j[r[p]] - j[r[p - 1]]) for p in range(len(r))])\n",
    "\n",
    "# Reverse the order of all elements from element i to element k in array r.\n",
    "two_opt_swap = lambda r,i,k: np.concatenate((r[0:i], r[k:- len(r) + i - 1:-1],r[k + 1:len(r)]))\n",
    "\n",
    "def two_opt(jobs,improvement_threshold): \n",
    "    route = np.arange(jobs.shape[0])\n",
    "    # Initialize the improvement factor.\n",
    "    improvement_factor = 1 \n",
    "    best_distance = path_distance(route,jobs)\n",
    "    \n",
    "    while improvement_factor > improvement_threshold: \n",
    "        # Record the distance at the beginning of the loop.\n",
    "        distance_to_beat = best_distance\n",
    "        \n",
    "        for swap_first in range(1,len(route)-2):\n",
    "            for swap_last in range(swap_first+1,len(route)): \n",
    "                # try reversing the order of these jobs\n",
    "                new_route = two_opt_swap(route,swap_first,swap_last) \n",
    "                # check the total distance with this modification.\n",
    "                new_distance = path_distance(new_route,jobs)\n",
    "                \n",
    "                if new_distance < best_distance: \n",
    "                    route = new_route\n",
    "                    best_distance = new_distance \n",
    "        # Calculate how much the route has improved.\n",
    "        improvement_factor = 1 - best_distance/distance_to_beat \n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47974282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-TSP\n",
    "def one_TSP(Gi, offset=0):\n",
    "    \"\"\"\n",
    "    Execute 1-TSP optimal algorithm on the give sub-graph_i\n",
    "    \n",
    "    Input:\n",
    "    Gi - sub-graph i\n",
    "    offset - the index to start from.\n",
    "    \"\"\"\n",
    "    R = 6371\n",
    "    lat_lst, lon_lst = Gi.get_jobs_location()\n",
    "    \n",
    "    lat_lst_radians = np.array([math.radians(lat) for lat in lat_lst])\n",
    "    lon_lst_radians = np.array([math.radians(lon) for lon in lon_lst])\n",
    "    lon_cos = np.array([math.cos(lon_r) for lon_r in lon_lst_radians])\n",
    "    x =  lon_cos * np.array([math.cos(lat_r) for lat_r in lat_lst_radians]) * R\n",
    "    y= lon_cos * np.array([math.sin(lat_r) for lat_r in lat_lst_radians]) * R\n",
    "    jobs_locations = pd.DataFrame({\"X\": x, \"Y\": y})\n",
    "    df = jobs_locations.copy()\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 100), copy=True)\n",
    "    scaled_df = scaler.fit_transform(df)\n",
    "    scaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2'])\n",
    "    \n",
    "    jobs_location = np.asarray(jobs_locations)\n",
    "    scaled = np.asarray(scaled_df)\n",
    "    \n",
    "    route = two_opt(scaled, 0.001)\n",
    "    \n",
    "    # update vertices IDC_index\n",
    "    for v, idx in zip(Gi.V, route):\n",
    "        v.index = idx + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8af7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 4 \n",
    "def m_TSP(G, number_of_resources):\n",
    "    \"\"\"\n",
    "    Algorithm 4 - achieve a good patition by AHP, \n",
    "        then execute 1-TSP on each sub-graph_i in the pratition.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    Gs = average_hamilton_partition(G, number_of_resources)\n",
    "    print('AHP - DONE!')\n",
    "    for Gi in Gs:\n",
    "        one_TSP(Gi, offset)\n",
    "        offset += Gi.length\n",
    "    return Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e319761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_partition(Gs, title):\n",
    "    \"\"\"\n",
    "    Ploting the partition Gs\n",
    "    \"\"\"\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(Gs) * 5))\n",
    "    colors = colors[::5]\n",
    "    for G, c in zip(Gs, colors):\n",
    "        lat_lst, lon_lst = G.get_jobs_location()\n",
    "        plt.scatter(lat_lst, lon_lst, color=c)\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the relevant columns\n",
    "lat_lst = df_unique['Latitude']\n",
    "lon_lst = df_unique['Longitude']\n",
    "# duration_lst = df_unique['DurationInMinutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph G\n",
    "number_of_jobs = df_unique.shape[0]\n",
    "V = []\n",
    "for idx, row in df_unique.iterrows():\n",
    "    V.append(Vertex(idx, lat_lst[idx], lon_lst[idx])) #, duration_lst[idx]))\n",
    "E = [Edge(V[i1], V[i2]) for i1 in range(number_of_jobs) for i2 in range(i1 + 1, number_of_jobs)]\n",
    "G = (V, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = average_hamilton_partition(G, number_of_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "for Gi in Gs:\n",
    "    one_TSP(Gi, offset)\n",
    "    offset += Gi.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_partition(Gs, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63670d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, G in enumerate(Gs):\n",
    "    print()\n",
    "    print(f'--------------- {idx} ---------------')\n",
    "    print(f'Size = {G.size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8228593",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "lat, lon = Gs[i].get_jobs_location()\n",
    "plt.plot(lat, lon)\n",
    "print(len(Gs[i].V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for G in Gs:\n",
    "    for v in G.V:\n",
    "        df.iloc[v.id, -1] = v.index\n",
    "        \n",
    "for key, values in dict_of_duplicates.items():\n",
    "    if values:\n",
    "        for value_idx in values:\n",
    "            df.iloc[value_idx, -1] = df.iloc[key, -1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(f'results-M-TSP_{name_of_file}_only_distances.xlsx', sheet_name='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f7313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
